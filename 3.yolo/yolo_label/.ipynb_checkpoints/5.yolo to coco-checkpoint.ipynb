{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4860c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "YOLO 格式的数据集转化为 COCO 格式的数据集\n",
    "--root_dir 输入根路径\n",
    "--save_path 保存文件的名字(没有random_split时使用)\n",
    "--random_split 有则会随机划分数据集，然后再分别保存为3个文件。\n",
    "--split_by_file 按照 ./train.txt ./val.txt ./test.txt 来对数据集进行划分。\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--root_dir', default=r'F:\\PythonWork\\dataset\\obj\\facilities_500kv', type=str,\n",
    "                    help=\"root path of images and labels, include ./images and ./labels and classes.txt\")\n",
    "parser.add_argument('--save_path', type=str, default=r'./train.json',\n",
    "                    help=\"if not split the dataset, give a path to a json file\")\n",
    "parser.add_argument('--random_split', action='store_true', help=\"random split the dataset, default ratio is 8:1:1\")\n",
    "parser.add_argument('--split_by_file', action='store_true',\n",
    "                    help=\"define how to split the dataset, include ./train.txt ./val.txt ./test.txt \")\n",
    "\n",
    "arg = parser.parse_args()\n",
    "\n",
    "\n",
    "# def train_test_val_split_random(img_paths, ratio_train=0.8, ratio_test=0.1, ratio_val=0.1):\n",
    "#     # 这里可以修改数据集划分的比例。\n",
    "#     assert int(ratio_train + ratio_test + ratio_val) == 1\n",
    "#     train_img, middle_img = train_test_split(img_paths, test_size=1 - ratio_train, random_state=233)\n",
    "#     ratio = ratio_val / (1 - ratio_train)\n",
    "#     val_img, test_img = train_test_split(middle_img, test_size=ratio, random_state=233)\n",
    "#     print(\"NUMS of train:val:test = {}:{}:{}\".format(len(train_img), len(val_img), len(test_img)))\n",
    "#     return train_img, val_img, test_img\n",
    "\n",
    "\n",
    "def train_test_val_split_by_files(img_paths, root_dir):\n",
    "    # 根据文件 train.txt, val.txt, test.txt（里面写的都是对应集合的图片名字） 来定义训练集、验证集和测试集\n",
    "    phases = ['train', 'val', 'test']\n",
    "    img_split = []\n",
    "    for p in phases:\n",
    "        define_path = os.path.join(root_dir, f'{p}.txt')\n",
    "        print(f'Read {p} dataset definition from {define_path}')\n",
    "        assert os.path.exists(define_path)\n",
    "        with open(define_path, 'r') as f:\n",
    "            img_paths = f.readlines()\n",
    "            # img_paths = [os.path.split(img_path.strip())[1] for img_path in img_paths]  # NOTE 取消这句备注可以读取绝对地址。\n",
    "            img_split.append(img_paths)\n",
    "    return img_split[0], img_split[1], img_split[2]\n",
    "\n",
    "\n",
    "def yolo2coco(arg):\n",
    "    root_path = arg.root_dir\n",
    "    print(\"Loading data from \", root_path)\n",
    "\n",
    "    assert os.path.exists(root_path)\n",
    "    originLabelsDir = os.path.join(root_path, 'labels/train')\n",
    "    originImagesDir = os.path.join(root_path, 'images/train')\n",
    "    with open(os.path.join(root_path, 'classes.txt')) as f:\n",
    "        classes = f.read().strip().split()\n",
    "    # images dir name\n",
    "    indexes = os.listdir(originImagesDir)\n",
    "\n",
    "    if arg.random_split or arg.split_by_file:\n",
    "        # 用于保存所有数据的图片信息和标注信息\n",
    "        train_dataset = {'categories': [], 'annotations': [], 'images': []}\n",
    "        val_dataset = {'categories': [], 'annotations': [], 'images': []}\n",
    "        test_dataset = {'categories': [], 'annotations': [], 'images': []}\n",
    "\n",
    "        # 建立类别标签和数字id的对应关系, 类别id从0开始。\n",
    "        for i, cls in enumerate(classes, 0):\n",
    "            train_dataset['categories'].append({'id': i, 'name': cls, 'supercategory': 'mark'})\n",
    "            val_dataset['categories'].append({'id': i, 'name': cls, 'supercategory': 'mark'})\n",
    "            test_dataset['categories'].append({'id': i, 'name': cls, 'supercategory': 'mark'})\n",
    "\n",
    "        if arg.random_split:\n",
    "            print(\"spliting mode: random split\")\n",
    "            train_img, val_img, test_img = train_test_val_split_random(indexes, 0.8, 0.1, 0.1)\n",
    "        elif arg.split_by_file:\n",
    "            print(\"spliting mode: split by files\")\n",
    "            train_img, val_img, test_img = train_test_val_split_by_files(indexes, root_path)\n",
    "    else:\n",
    "        dataset = {'categories': [], 'annotations': [], 'images': []}\n",
    "        for i, cls in enumerate(classes, 0):\n",
    "            dataset['categories'].append({'id': i, 'name': cls, 'supercategory': 'mark'})\n",
    "\n",
    "    # 标注的id\n",
    "    ann_id_cnt = 0\n",
    "    for k, index in enumerate(tqdm(indexes)):\n",
    "        # 支持 png jpg 格式的图片。\n",
    "        txtFile = index.replace('images', 'txt').replace('.JPG', '.txt').replace('.png', '.txt')\n",
    "        # 读取图像的宽和高\n",
    "        im = cv2.imread(os.path.join(root_path, 'images/train/') + index)\n",
    "        height, width, _ = im.shape\n",
    "        if arg.random_split or arg.split_by_file:\n",
    "            # 切换dataset的引用对象，从而划分数据集\n",
    "            if index in train_img:\n",
    "                dataset = train_dataset\n",
    "            elif index in val_img:\n",
    "                dataset = val_dataset\n",
    "            elif index in test_img:\n",
    "                dataset = test_dataset\n",
    "        # 添加图像的信息\n",
    "        dataset['images'].append({'file_name': index,\n",
    "                                  'id': k,\n",
    "                                  'width': width,\n",
    "                                  'height': height})\n",
    "        if not os.path.exists(os.path.join(originLabelsDir, txtFile)):\n",
    "            # 如没标签，跳过，只保留图片信息。\n",
    "            continue\n",
    "        with open(os.path.join(originLabelsDir, txtFile), 'r') as fr:\n",
    "            labelList = fr.readlines()\n",
    "            for label in labelList:\n",
    "                label = label.strip().split()\n",
    "                x = float(label[1])\n",
    "                y = float(label[2])\n",
    "                w = float(label[3])\n",
    "                h = float(label[4])\n",
    "\n",
    "                # convert x,y,w,h to x1,y1,x2,y2\n",
    "                H, W, _ = im.shape\n",
    "                x1 = (x - w / 2) * W\n",
    "                y1 = (y - h / 2) * H\n",
    "                x2 = (x + w / 2) * W\n",
    "                y2 = (y + h / 2) * H\n",
    "                # 标签序号从0开始计算, coco2017数据集标号混乱，不管它了。\n",
    "                cls_id = int(label[0])\n",
    "                width = max(0, x2 - x1)\n",
    "                height = max(0, y2 - y1)\n",
    "                dataset['annotations'].append({\n",
    "                    'area': width * height,\n",
    "                    'bbox': [x1, y1, width, height],\n",
    "                    'category_id': cls_id,\n",
    "                    'id': ann_id_cnt,\n",
    "                    'image_id': k,\n",
    "                    'iscrowd': 0,\n",
    "                    # mask, 矩形是从左上角点按顺时针的四个顶点\n",
    "                    'segmentation': [[x1, y1, x2, y1, x2, y2, x1, y2]]\n",
    "                })\n",
    "                ann_id_cnt += 1\n",
    "\n",
    "    # 保存结果\n",
    "    folder = os.path.join(root_path, 'annotations')\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    if arg.random_split or arg.split_by_file:\n",
    "        for phase in ['train', 'val', 'test']:\n",
    "            json_name = os.path.join(root_path, 'annotations/{}.json'.format(phase))\n",
    "            with open(json_name, 'w') as f:\n",
    "                if phase == 'train':\n",
    "                    json.dump(train_dataset, f)\n",
    "                elif phase == 'val':\n",
    "                    json.dump(val_dataset, f)\n",
    "                elif phase == 'test':\n",
    "                    json.dump(test_dataset, f)\n",
    "            print('Save annotation to {}'.format(json_name))\n",
    "    else:\n",
    "        json_name = os.path.join(root_path, 'annotations/{}'.format(arg.save_path))\n",
    "        with open(json_name, 'w') as f:\n",
    "            json.dump(dataset, f)\n",
    "            print('Save annotation to {}'.format(json_name))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    yolo2coco(arg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
